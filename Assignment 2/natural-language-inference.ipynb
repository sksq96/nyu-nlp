{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN/CNN-based Natural Language Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboard import TensorBoard\n",
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.train_file = 'dev.tsv'\n",
    "params.val_file = 'dev.tsv'\n",
    "params.word_vectors = 'glove.6B.300d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Namespace()\n",
    "\n",
    "params.epochs = 10\n",
    "params.batch_size = 128\n",
    "params.encoder = 'rnn'\n",
    "params.d_embed = 300\n",
    "params.word_vectors = 'fasttext.en.300d'\n",
    "params.d_hidden = 300\n",
    "params.d_fc = 100\n",
    "params.n_layers = 1\n",
    "params.lr = .001\n",
    "params.dp_ratio = 0.2\n",
    "params.gpu = 0\n",
    "params.train_file = 'snli_train.tsv'\n",
    "params.val_file = 'snli_val.tsv'\n",
    "params.log_every = 50\n",
    "params.dev_every = 1000\n",
    "params.experiment = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.params = params\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            params.d_embed, params.d_hidden, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            params.d_hidden, params.d_hidden, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input is: (length, batch_size, num_channels)\n",
    "        # conv1d module expects: (batch_size, num_channels, length)\n",
    "        h0 = x.transpose(0, 1).transpose(1, 2).contiguous()\n",
    "\n",
    "        h0 = self.relu(self.conv1(h0))\n",
    "        h0 = self.relu(self.conv2(h0))\n",
    "\n",
    "        # return (batch_size, num_channels)\n",
    "        h0 = h0.transpose(1, 2)\n",
    "        return torch.sum(h0, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        self.params = params\n",
    "        input_size = params.d_embed\n",
    "        dropout = 0 if params.n_layers == 1 else params.dp_ratio\n",
    "        self.rnn = nn.GRU(input_size=input_size, hidden_size=params.d_hidden,\n",
    "                        num_layers=params.n_layers, dropout=dropout,\n",
    "                        bidirectional=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size()[1]\n",
    "        state_shape = self.params.n_cells, batch_size, self.params.d_hidden\n",
    "        h0 =  inputs.new_zeros(state_shape)\n",
    "        _, ht = self.rnn(inputs, h0)\n",
    "\n",
    "        # bring batch_size to the 0th dim\n",
    "        ht = ht[-2:].transpose(0, 1).contiguous()\n",
    "        # concat forward and backward rnn hidden\n",
    "        return ht.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLI(nn.Module):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(NLI, self).__init__()\n",
    "        \n",
    "        self.params = params\n",
    "        self.embed = nn.Embedding(params.n_embed, params.d_embed)\n",
    "        if params.encoder == 'rnn':\n",
    "            self.encoder = RNNEncoder(params)\n",
    "        elif params.encoder == 'cnn':\n",
    "            self.encoder = CNNEncoder(params)\n",
    "        else:\n",
    "            raise ValueError(f'Encoder {params.encoder} is not supported. Try using cnn or rnn.')\n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=params.dp_ratio)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        fc_in_size = params.d_hidden\n",
    "        # concat s1 and s2\n",
    "        fc_in_size *= 2\n",
    "        if params.encoder == 'rnn':\n",
    "            # concat forward and backward bi-rnn\n",
    "            fc_in_size *= 2\n",
    "\n",
    "        fc_ot_size = params.d_fc\n",
    "        \n",
    "        # 2-layers fc\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(fc_in_size, fc_ot_size),\n",
    "            self.relu,\n",
    "            self.dropout,\n",
    "            nn.Linear(fc_ot_size, params.d_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        # fix embeddings, do not backprop\n",
    "        s1_embed = self.embed(s1).detach()\n",
    "        s2_embed = self.embed(s2).detach()\n",
    "        \n",
    "        s1_encode = self.encoder(s1_embed)\n",
    "        s2_encode = self.encoder(s2_embed)\n",
    "        \n",
    "        return self.out(torch.cat([s1_encode, s2_encode], 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu business\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(params.gpu)\n",
    "    device = torch.device('cuda:{}'.format(params.gpu))\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensoboard logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = f\"runs/{params.experiment}/{time.asctime(time.localtime())}/\"\n",
    "tb = TensorBoard(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define text felids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Other tokenizers?\n",
    "inputs = data.Field(lower=True, tokenize='spacy')\n",
    "answers = data.Field(sequential=False, unk_token=None)\n",
    "\n",
    "train, valid = data.TabularDataset.splits(\n",
    "    path=\"data\",\n",
    "    train=params.train_file, validation=params.val_file,\n",
    "    format='tsv',\n",
    "    skip_header=True,\n",
    "    fields=[(\"sentence1\", inputs), (\"sentence2\", inputs), (\"label\", answers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build vocabulary and load per-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Too slow for big dataset. Use n-workers.\n",
    "inputs.build_vocab(train, valid, vectors=params.word_vectors)\n",
    "answers.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent of dataloader in torchtext\n",
    "train_iter, valid_iter = data.BucketIterator.splits(\n",
    "            (train, valid), \n",
    "            batch_size=params.batch_size, \n",
    "            sort_key=lambda x: len(x.sentence1),\n",
    "            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.n_embed = len(inputs.vocab)\n",
    "params.d_out = len(answers.vocab)\n",
    "# double the number of cells for bidirectional networks\n",
    "params.n_cells = params.n_layers * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLI(\n",
       "  (embed): Embedding(2640, 100)\n",
       "  (encoder): RNNEncoder(\n",
       "    (rnn): GRU(100, 300, bidirectional=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2)\n",
       "  (relu): ReLU()\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=1200, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2)\n",
       "    (3): Linear(in_features=100, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NLI(params)\n",
    "# hack to tie pre-trained vectors to Embedding class\n",
    "# TODO: find a cleaner way\n",
    "if params.word_vectors:\n",
    "    model.embed.weight.data.copy_(inputs.vocab.vectors)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1108003.\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of parameters: {}.\".format(sum(p.numel()\n",
    "                                                   for p in model.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=params.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global step for tensorboard logging\n",
    "step = 0\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    # logging business\n",
    "    header = '  Time Epoch Iteration Progress    (%Epoch)   Loss       Accuracy'\n",
    "    print(header)\n",
    "\n",
    "    global step\n",
    "    for epoch in range(params.epochs):\n",
    "        train_iter.init_epoch()\n",
    "        n_correct, n_total = 0, 0\n",
    "        for batch_idx, batch in enumerate(train_iter):\n",
    "            step += 1\n",
    "\n",
    "            model.train()\n",
    "            opt.zero_grad()\n",
    "            answer = model(batch.sentence1, batch.sentence2)\n",
    "            loss = criterion(answer, batch.label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # evaluate performance on validation set periodically\n",
    "            if step % params.dev_every == 0:\n",
    "                validation()\n",
    "\n",
    "            if step % params.log_every == 0:\n",
    "                # calculate accuracy\n",
    "                n_correct += (torch.max(answer, 1)[1].view(batch.label.size()) == batch.label).sum().item()\n",
    "                n_total += batch.batch_size\n",
    "                accuracy = 100. * n_correct/n_total\n",
    "\n",
    "                # print progress message\n",
    "                log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:12.4f}'.split(','))\n",
    "                print(log_template.format(time.time()-start, epoch, step, 1+batch_idx, len(train_iter),\n",
    "                        100. * (1+batch_idx) / len(train_iter), loss.item(), accuracy))\n",
    "                \n",
    "                if tb is not None:\n",
    "                    tb.scalar_summary(\"train/loss\", loss.item(), step)\n",
    "                    tb.scalar_summary(\"train/accuracy\", accuracy, step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    global step\n",
    "    # switch model to evaluation mode\n",
    "    model.eval()\n",
    "    valid_iter.init_epoch()\n",
    "\n",
    "    # calculate accuracy on validation set\n",
    "    n_valid_correct, valid_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for _, valid_batch in enumerate(valid_iter):\n",
    "            answer = model(valid_batch.sentence1, valid_batch.sentence2)\n",
    "            n_valid_correct += (torch.max(answer, 1)[1].view(valid_batch.label.size()) == valid_batch.label).sum().item()\n",
    "            valid_loss = criterion(answer, valid_batch.label)\n",
    "    valid_acc = 100. * n_valid_correct / len(valid)\n",
    "\n",
    "    valid_log_template = 'Validation Loss: {:>8.6f}, Accuracy: {:12.4f}'\n",
    "    print(valid_log_template.format(valid_loss.item(), valid_acc))\n",
    "\n",
    "    if tb is not None:\n",
    "        tb.scalar_summary(\"validation/loss\", valid_loss.item(), step)\n",
    "        tb.scalar_summary(\"validation/accuracy\", valid_acc, step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.numel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1108003"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, d_embed=300, d_fc=100, d_hidden=300, dev_every=1000, dp_ratio=0.2, encoder='rnn', epochs=10, experiment='test', gpu=0, log_every=50, lr=0.001, n_layers=1, train_file='snli_train.tsv', val_file='snli_val.tsv', word_vectors='fasttext.en.300d')\n"
     ]
    }
   ],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time Epoch Iteration Progress    (%Epoch)   Loss       Accuracy\n",
      "    51     6        50     2/8          25% 1.031085      48.4375\n",
      "    85    12       100     4/8          50% 0.781582      67.1875\n",
      "   118    18       150     6/8          75% 0.536626      81.2500\n",
      "   150    24       200     8/8         100% 0.208219      93.7500\n",
      "   183    31       250     2/8          25% 0.022324     100.0000\n",
      "   215    37       300     4/8          50% 0.003283     100.0000\n"
     ]
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Multi-Genre NLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split MNLI\n",
    "\n",
    "Split MNLI into 5 different files. Then use these files as validation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mnli_val.tsv\", sep='\\t')\n",
    "\n",
    "for k, v in df.groupby(\"genre\"):\n",
    "    v[[\"sentence1\", \"sentence2\", \"label\"]].to_csv(f\"mnli_{k}.tsv\", sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.val_file = 'mnli_telephone.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Other tokenizers?\n",
    "inputs = data.Field(lower=True, tokenize='spacy')\n",
    "answers = data.Field(sequential=False, unk_token=None)\n",
    "\n",
    "train, valid = data.TabularDataset.splits(\n",
    "    path=\"data\",\n",
    "    train=params.train_file, validation=params.val_file,\n",
    "    format='tsv',\n",
    "    skip_header=True,\n",
    "    fields=[(\"sentence1\", inputs), (\"sentence2\", inputs), (\"label\", answers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 398811/400000 [00:40<00:00, 13099.01it/s]"
     ]
    }
   ],
   "source": [
    "# TODO: Too slow for big dataset. Use n-workers.\n",
    "inputs.build_vocab(train, valid, vectors=params.word_vectors)\n",
    "answers.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent of dataloader in torchtext\n",
    "train_iter, valid_iter = data.BucketIterator.splits(\n",
    "            (train, valid), \n",
    "            batch_size=params.batch_size, \n",
    "            sort_key=lambda x: len(x.sentence1),\n",
    "            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.n_embed = len(inputs.vocab)\n",
    "params.d_out = len(answers.vocab)\n",
    "# double the number of cells for bidirectional networks\n",
    "params.n_cells = params.n_layers * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.098440, Accuracy: 44.6326\n"
     ]
    }
   ],
   "source": [
    "validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
